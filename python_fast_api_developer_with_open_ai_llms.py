# -*- coding: utf-8 -*-
"""Python-Fast-API-Developer-with-Open-AI-LLMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rmwthC9VE7ef9rABCo1IQvqfWw2st0Fu

##Step 1: Setting the Stage
Let’s start by importing the necessary libraries and setting up the main components of our system.
"""

# Importing required modules
from langchain.llms import OpenAI
from langchain.memory import ConversationSummaryBufferMemory
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
import uvicorn
import requests
import os
import shutil
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain import hub
from langchain_core.runnables import RunnablePassthrough

"""##Step 2: Initializing Fast API and OpenAI
Now, let’s set up our Fast API instance and initialize the OpenAI language model.
"""

# Create an instance of OpenAI language model
# please your own OPEN API Key
OPENAI_API_KEY = os.getenv(“OPENAI_API_KEY”)
prompt = hub.pull(“rlm/rag-prompt”)
llm = ChatOpenAI(model_name=”gpt-3.5-turbo”, temperature=0)
# Initializing FastAPI
app = FastAPI()

"""##Step 3: Handling File Uploads with Fast API
Let’s create an API endpoint for users to upload PDF documents.
"""

# API endpoint for file upload
@app.post(“/upload”)
async def upload_file(user_id: str, file: UploadFile = File(…)):
 try:
 contents = await file.read()
 with open(‘files/file_{}.pdf’.format(user_id), ‘wb’) as gf:
 gf.write(contents)
 except requests.exceptions.RequestException as err:
 return {“error”: f”Error occurred during file upload: {str(err)}”}

"""##Step 4: Document Processing and FAISS Indexing
Now, let’s continue with document processing and create FAISS indexes for efficient document retrieval.
"""

# Document processing and FAISS indexing
@app.post(“/upload”)
async def upload_file(user_id: str, file: UploadFile = File(…)):
 try:
 contents = await file.read()
 with open(‘files/file_{}.pdf’.format(user_id), ‘wb’) as gf:
 gf.write(contents)
 except requests.exceptions.RequestException as err:
 return {“error”: f”Error occurred during file upload: {str(err)}”}

 try:
 loader = PyPDFLoader(“files/file_{}.pdf”.format(user_id))
 pages = loader.load_and_split()
 chunks = pages
 db_name = “db_{}”.format(user_id)
 dbs[db_name] = FAISS.from_documents(chunks, embeddings)
 dbs[db_name].save_local(“faiss_db/faiss_{}”.format(db_name))
 os.remove(“files/file_{}.pdf”.format(user_id))
 return {“filename”: f”file uploaded successfully for user {user_id}”}
 except requests.exceptions.RequestException as err:
 return JSONResponse(status_code=400, content={“error”: f”Error occurred during file upload: {str(err)}”})

"""##Step 5: Asking Questions with Conversational Retrieval
Now, let’s implement an API endpoint for asking questions and retrieving answers using the RAG Chain.
"""

# Asking questions API endpoint
@app.post(“/askqa/{user_id}”)
async def askqa(user_id: str, query: str = ‘summarize this document’ ):
 try:
   if user_id not in memory:
   memory[user_id] = ConversationSummaryBufferMemory(
   llm=llm,
   output_key=’answer’,
   memory_key=’chat_history’,
   return_messages=True
   )
   db_name = “faiss_db_” + user_id
   new_db = FAISS.load_local(f”faiss_db/{db_name}”, embeddings)
   except FileNotFoundError as file_err:
   return JSONResponse(status_code=404, content={‘error’: f”File not found. for user {user_id} First upload the docs: {str(file_err)}”})

# initialize the RAG Chain
retriever = new_db.as_retriever()
 rag_chain = (
   {"context": retriever | format_docs, "question": RunnablePassthrough()}
   | prompt
   | llm
   | StrOutputParser()
 )

try:
 prompts[user_id] = query
 suggestion = rag_chain.invoke("write prompt for the suggestion")
 answer = rag_chain.invoke(prompts[user_id])
 return {'answer': answer,'suggestion':suggestion}
 except requests.exceptions.HTTPError as err:
 return JSONResponse(status_code=404, content={'error': str(err)})

"""##Step 6: Clearing Cache for Users
Now, let’s implement API endpoints to clear the cache for specific users or all users.
"""

# Clear cache for a specific user
@app.get(“/clearall-user/{user_id}”)
async def clearall_user(user_id: str):
 try:
 db_name = “db_” + user_id
 shutil.rmtree(f”faiss_db/faiss_{db_name}”)
 if user_id in memory:
 del memory[user_id]
 del dbs[db_name]
 del prompt[user_id]
 return JSONResponse(content={‘message’: f’Cache cleared for user {user_id}’})
 except FileNotFoundError as file_err:
 return JSONResponse(status_code=404, content={‘error’: f”Cache does not exist for user {user_id}: {str(file_err)}”})

"""##Step 7: Clearing Cache for All Users
Now, let’s implement an endpoint to clear the cache for all users.
"""

# Clear cache for all users
@app.get(“/clearall”)
async def clear_all():
 try:
 shutil.rmtree(“faiss_db”)
 memory.clear()
 dbs.clear()
 return JSONResponse(content={‘message’: ‘All caches cleared’})
 except FileNotFoundError as file_err:
 return JSONResponse(status_code=404, content={‘error’: f”All caches have already been cleared: {str(file_err)}”})

"""##Step 9: Testing and Deployment
With the implementation complete, it’s time to test the system and deploy it. Ensure all dependencies are installed, and run the FastAPI app using the following command:
"""

uvicorn your_script_name:app — host 0.0.0.0 — port 8000

"""You can now test the endpoints using tools like [Swagger UI](http://localhost:8000/docs) or [FastAPI’s ReDoc](http://localhost:8000/redoc)."""